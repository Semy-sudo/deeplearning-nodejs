{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengal 파일 길이: 9996\n",
      "Bengal  :  ./catdataset/train/Bengal\\Bengal (1).jpg\n",
      "Bengal  :  ./catdataset/train/Bengal\\Bengal (717).jpg\n",
      "Bengal  :  ./catdataset/train/Bengal\\Bengal_0_1368.jpg\n",
      "Bengal  :  ./catdataset/train/Bengal\\Bengal_0_2015.jpg\n",
      "Bengal  :  ./catdataset/train/Bengal\\Bengal_0_2666.jpg\n",
      "Bengal  :  ./catdataset/train/Bengal\\Bengal_0_3317.jpg\n",
      "Bengal  :  ./catdataset/train/Bengal\\Bengal_0_3966.jpg\n",
      "Bengal  :  ./catdataset/train/Bengal\\Bengal_0_4611.jpg\n",
      "Bengal  :  ./catdataset/train/Bengal\\Bengal_0_5256.jpg\n",
      "Bengal  :  ./catdataset/train/Bengal\\Bengal_0_5906.jpg\n",
      "Bengal  :  ./catdataset/train/Bengal\\Bengal_0_6550.jpg\n",
      "Bengal  :  ./catdataset/train/Bengal\\Bengal_0_7193.jpg\n",
      "Bengal  :  ./catdataset/train/Bengal\\Bengal_0_7837.jpg\n",
      "Bengal  :  ./catdataset/train/Bengal\\Bengal_0_8481.jpg\n",
      "Bengal  :  ./catdataset/train/Bengal\\Bengal_0_9125.jpg\n",
      "Russianblue 파일 길이: 10000\n",
      "Russianblue  :  ./catdataset/train/Russianblue\\Russian (1).jpg\n",
      "Russianblue  :  ./catdataset/train/Russianblue\\Russian (729).jpg\n",
      "Russianblue  :  ./catdataset/train/Russianblue\\Russianblue_0_140.jpg\n",
      "Russianblue  :  ./catdataset/train/Russianblue\\Russianblue_0_2090.jpg\n",
      "Russianblue  :  ./catdataset/train/Russianblue\\Russianblue_0_2802.jpg\n",
      "Russianblue  :  ./catdataset/train/Russianblue\\Russianblue_0_3498.jpg\n",
      "Russianblue  :  ./catdataset/train/Russianblue\\Russianblue_0_4196.jpg\n",
      "Russianblue  :  ./catdataset/train/Russianblue\\Russianblue_0_4885.jpg\n",
      "Russianblue  :  ./catdataset/train/Russianblue\\Russianblue_0_5584.jpg\n",
      "Russianblue  :  ./catdataset/train/Russianblue\\Russianblue_0_6285.jpg\n",
      "Russianblue  :  ./catdataset/train/Russianblue\\Russianblue_0_70.jpg\n",
      "Russianblue  :  ./catdataset/train/Russianblue\\Russianblue_0_7687.jpg\n",
      "Russianblue  :  ./catdataset/train/Russianblue\\Russianblue_0_8392.jpg\n",
      "Russianblue  :  ./catdataset/train/Russianblue\\Russianblue_0_9093.jpg\n",
      "Russianblue  :  ./catdataset/train/Russianblue\\Russianblue_0_9790.jpg\n",
      "Bombay 파일 길이: 10000\n",
      "Bombay  :  ./catdataset/train/Bombay\\Bombay (1).jpg\n",
      "Bombay  :  ./catdataset/train/Bombay\\Bombay (73).jpg\n",
      "Bombay  :  ./catdataset/train/Bombay\\Bombay_0_1362.jpg\n",
      "Bombay  :  ./catdataset/train/Bombay\\Bombay_0_1998.jpg\n",
      "Bombay  :  ./catdataset/train/Bombay\\Bombay_0_2634.jpg\n",
      "Bombay  :  ./catdataset/train/Bombay\\Bombay_0_3269.jpg\n",
      "Bombay  :  ./catdataset/train/Bombay\\Bombay_0_3902.jpg\n",
      "Bombay  :  ./catdataset/train/Bombay\\Bombay_0_4536.jpg\n",
      "Bombay  :  ./catdataset/train/Bombay\\Bombay_0_5171.jpg\n",
      "Bombay  :  ./catdataset/train/Bombay\\Bombay_0_5807.jpg\n",
      "Bombay  :  ./catdataset/train/Bombay\\Bombay_0_6444.jpg\n",
      "Bombay  :  ./catdataset/train/Bombay\\Bombay_0_7080.jpg\n",
      "Bombay  :  ./catdataset/train/Bombay\\Bombay_0_7713.jpg\n",
      "Bombay  :  ./catdataset/train/Bombay\\Bombay_0_8348.jpg\n",
      "Bombay  :  ./catdataset/train/Bombay\\Bombay_0_8988.jpg\n",
      "ragdoll 파일 길이: 9999\n",
      "ragdoll  :  ./catdataset/train/ragdoll\\Ragdoll (1).jpg\n",
      "ragdoll  :  ./catdataset/train/ragdoll\\Ragdoll (746).jpg\n",
      "ragdoll  :  ./catdataset/train/ragdoll\\ragdoll_0_1670.jpg\n",
      "ragdoll  :  ./catdataset/train/ragdoll\\ragdoll_0_2352.jpg\n",
      "ragdoll  :  ./catdataset/train/ragdoll\\ragdoll_0_2989.jpg\n",
      "ragdoll  :  ./catdataset/train/ragdoll\\ragdoll_0_3625.jpg\n",
      "ragdoll  :  ./catdataset/train/ragdoll\\ragdoll_0_438.jpg\n",
      "ragdoll  :  ./catdataset/train/ragdoll\\ragdoll_0_5363.jpg\n",
      "ragdoll  :  ./catdataset/train/ragdoll\\ragdoll_0_6000.jpg\n",
      "ragdoll  :  ./catdataset/train/ragdoll\\ragdoll_0_6637.jpg\n",
      "ragdoll  :  ./catdataset/train/ragdoll\\ragdoll_0_7273.jpg\n",
      "ragdoll  :  ./catdataset/train/ragdoll\\ragdoll_0_791.jpg\n",
      "ragdoll  :  ./catdataset/train/ragdoll\\ragdoll_0_8546.jpg\n",
      "ragdoll  :  ./catdataset/train/ragdoll\\ragdoll_0_9182.jpg\n",
      "ragdoll  :  ./catdataset/train/ragdoll\\ragdoll_0_9819.jpg\n",
      "ok 39995\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os,glob, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "caltech_dir = \"./catdataset/train\"\n",
    "categories = [\"Bengal\", \"Russianblue\",\"Bombay\",\"ragdoll\"]  ##카테고리 설정\n",
    "nb_classes = len(categories)\n",
    "\n",
    "image_w=128\n",
    "image_h=128\n",
    "\n",
    "pixels=image_h*image_w*3\n",
    "\n",
    "X=[]\n",
    "y=[]\n",
    "\n",
    "for idx, cat in enumerate(categories):\n",
    "    \n",
    "    label=[0 for i in range(nb_classes)]\n",
    "    label[idx]=1\n",
    "    \n",
    "    image_dir =caltech_dir +\"/\"+cat\n",
    "    files=glob.glob(image_dir+\"/*.jpg\")\n",
    "    print(cat,\"파일 길이:\",len(files))\n",
    "    for i,f in enumerate(files):\n",
    "        img = Image.open(f)\n",
    "        img=img.convert(\"RGB\")\n",
    "        img = img.resize((image_w, image_h))\n",
    "        data = np.asarray(img)\n",
    "\n",
    "        X.append(data)   \n",
    "        y.append(label)\n",
    "        \n",
    "        if i % 700 == 0:   ##???\n",
    "            print(cat, \" : \", f)\n",
    "\n",
    "X = np.array(X) ##배열 \n",
    "y = np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)   ##그래프 표현을 위한 설정\n",
    "xy = (X_train, X_test, y_train, y_test)\n",
    "np.save(\"./numpy_data/m_image_data.npy\", xy)  \n",
    "\n",
    "print(\"ok\", len(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29996, 128, 128, 3)\n",
      "29996\n"
     ]
    }
   ],
   "source": [
    "import os, glob, numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "   \n",
    "\n",
    "import tensorflow as tf   #GPU 구현에 있어서 메모리를 절약해서 사용하기 위한 코드\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "X_train, X_test, y_train, y_test = np.load('./numpy_data/m_image_data.npy',allow_pickle=True)\n",
    "print(X_train.shape)\n",
    "print(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "categories = [\"Bombay\", \"Russianblue\",\"Bengal\",\"ragdoll\"]  ##카데고리 설정\n",
    "nb_classes = len(categories)\n",
    "\n",
    "X_train = X_train.astype('float64')/255\n",
    "X_test = X_test.astype('float64')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    model = Sequential()##선형계층\n",
    "    model.add(Conv2D(16, kernel_size=(3, 3),padding=\"same\", activation='relu', input_shape=X_train.shape[1:]))\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.125))\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),padding=\"same\", activation='relu'))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3),padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.125))\n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size=(3, 3),padding=\"same\", activation='relu'))\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3),padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.125))\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=(3, 3),padding=\"same\", activation='relu'))\n",
    "    model.add(Conv2D(256, kernel_size=(3, 3),padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.125))\n",
    "    \n",
    "    model.add(Conv2D(256, kernel_size=(3, 3),padding=\"same\", activation='relu'))\n",
    "    model.add(Conv2D(512, kernel_size=(3, 3),padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.125))\n",
    "    \n",
    "    model.add(Conv2D(512, kernel_size=(3, 3),padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.125))\n",
    "    \n",
    "              \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(nb_classes, activation='softmax')) ##soft max->0부터1사이의 출력값은 실수이며 출력의 총합을 1로 표현 확률을 표현할때 사용\n",
    "   \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) ##optimization ->딥러닝에서 학습속도를 빠르고 안정적이게 하는것  \n",
    "    model_dir = './model'\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    \n",
    "    model_path = model_dir + '/my_classification.h5'\n",
    "    checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True) ##keras 모델 체크포인트 save_best_only->정확도의 최고값을 갱신할때만 사용\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10) ##학습 종료 patience 성능이 증가하지 않는 epoch를 몇번이나 허용할 것인가 정의\n",
    "    ##최적의 데이터를 학습하면 더이상 학습을 하지 않는다\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 128, 128, 16)      448       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 5,897,860\n",
      "Trainable params: 5,897,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "94/94 [==============================] - ETA: 0s - loss: 1.3457 - accuracy: 0.2945 \n",
      "Epoch 00001: val_loss improved from inf to 1.11120, saving model to ./model\\my_classification.h5\n",
      "94/94 [==============================] - 1019s 11s/step - loss: 1.3457 - accuracy: 0.2945 - val_loss: 1.1112 - val_accuracy: 0.4425\n",
      "Epoch 2/50\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.8807 - accuracy: 0.5580 \n",
      "Epoch 00002: val_loss improved from 1.11120 to 0.55428, saving model to ./model\\my_classification.h5\n",
      "94/94 [==============================] - 989s 11s/step - loss: 0.8807 - accuracy: 0.5580 - val_loss: 0.5543 - val_accuracy: 0.7258\n",
      "Epoch 3/50\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.5377 - accuracy: 0.7448 \n",
      "Epoch 00003: val_loss improved from 0.55428 to 0.41340, saving model to ./model\\my_classification.h5\n",
      "94/94 [==============================] - 985s 10s/step - loss: 0.5377 - accuracy: 0.7448 - val_loss: 0.4134 - val_accuracy: 0.8230\n",
      "Epoch 4/50\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.3502 - accuracy: 0.8681 \n",
      "Epoch 00004: val_loss improved from 0.41340 to 0.24752, saving model to ./model\\my_classification.h5\n",
      "94/94 [==============================] - 988s 11s/step - loss: 0.3502 - accuracy: 0.8681 - val_loss: 0.2475 - val_accuracy: 0.9088\n",
      "Epoch 5/50\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2206 - accuracy: 0.9254 \n",
      "Epoch 00005: val_loss improved from 0.24752 to 0.16350, saving model to ./model\\my_classification.h5\n",
      "94/94 [==============================] - 985s 10s/step - loss: 0.2206 - accuracy: 0.9254 - val_loss: 0.1635 - val_accuracy: 0.9463\n",
      "Epoch 6/50\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1787 - accuracy: 0.9412 \n",
      "Epoch 00006: val_loss improved from 0.16350 to 0.15149, saving model to ./model\\my_classification.h5\n",
      "94/94 [==============================] - 990s 11s/step - loss: 0.1787 - accuracy: 0.9412 - val_loss: 0.1515 - val_accuracy: 0.9453\n",
      "Epoch 7/50\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1284 - accuracy: 0.9578 \n",
      "Epoch 00007: val_loss improved from 0.15149 to 0.14321, saving model to ./model\\my_classification.h5\n",
      "94/94 [==============================] - 989s 11s/step - loss: 0.1284 - accuracy: 0.9578 - val_loss: 0.1432 - val_accuracy: 0.9500\n",
      "Epoch 8/50\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0908 - accuracy: 0.9714 \n",
      "Epoch 00008: val_loss improved from 0.14321 to 0.09714, saving model to ./model\\my_classification.h5\n",
      "94/94 [==============================] - 981s 10s/step - loss: 0.0908 - accuracy: 0.9714 - val_loss: 0.0971 - val_accuracy: 0.9692\n",
      "Epoch 9/50\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9771 \n",
      "Epoch 00009: val_loss did not improve from 0.09714\n",
      "94/94 [==============================] - 985s 10s/step - loss: 0.0716 - accuracy: 0.9771 - val_loss: 0.1181 - val_accuracy: 0.9685\n",
      "Epoch 10/50\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0619 - accuracy: 0.9803 \n",
      "Epoch 00010: val_loss did not improve from 0.09714\n",
      "94/94 [==============================] - 986s 10s/step - loss: 0.0619 - accuracy: 0.9803 - val_loss: 0.1093 - val_accuracy: 0.9645\n",
      "Epoch 11/50\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0562 - accuracy: 0.9808 \n",
      "Epoch 00011: val_loss improved from 0.09714 to 0.07195, saving model to ./model\\my_classification.h5\n",
      "94/94 [==============================] - 992s 11s/step - loss: 0.0562 - accuracy: 0.9808 - val_loss: 0.0719 - val_accuracy: 0.9782\n",
      "Epoch 12/50\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9860 \n",
      "Epoch 00012: val_loss did not improve from 0.07195\n",
      "94/94 [==============================] - 989s 11s/step - loss: 0.0456 - accuracy: 0.9860 - val_loss: 0.0982 - val_accuracy: 0.9692\n",
      "Epoch 13/50\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0419 - accuracy: 0.9857 \n",
      "Epoch 00013: val_loss improved from 0.07195 to 0.06108, saving model to ./model\\my_classification.h5\n",
      "94/94 [==============================] - 984s 10s/step - loss: 0.0419 - accuracy: 0.9857 - val_loss: 0.0611 - val_accuracy: 0.9837\n",
      "Epoch 14/50\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 0.9837 \n",
      "Epoch 00014: val_loss did not improve from 0.06108\n",
      "94/94 [==============================] - 985s 10s/step - loss: 0.0472 - accuracy: 0.9837 - val_loss: 0.0732 - val_accuracy: 0.9785\n",
      "Epoch 15/50\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9895 \n",
      "Epoch 00015: val_loss did not improve from 0.06108\n",
      "94/94 [==============================] - 990s 11s/step - loss: 0.0283 - accuracy: 0.9895 - val_loss: 0.0667 - val_accuracy: 0.9822\n",
      "Epoch 16/50\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 0.9899 \n",
      "Epoch 00016: val_loss did not improve from 0.06108\n",
      "94/94 [==============================] - 987s 11s/step - loss: 0.0303 - accuracy: 0.9899 - val_loss: 0.0689 - val_accuracy: 0.9830\n",
      "Epoch 17/50\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9925 \n",
      "Epoch 00017: val_loss did not improve from 0.06108\n",
      "94/94 [==============================] - 988s 11s/step - loss: 0.0214 - accuracy: 0.9925 - val_loss: 0.0649 - val_accuracy: 0.9813\n",
      "Epoch 18/50\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9929 \n",
      "Epoch 00018: val_loss did not improve from 0.06108\n",
      "94/94 [==============================] - 991s 11s/step - loss: 0.0199 - accuracy: 0.9929 - val_loss: 0.0688 - val_accuracy: 0.9810\n",
      "Epoch 19/50\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9929 \n",
      "Epoch 00019: val_loss did not improve from 0.06108\n",
      "94/94 [==============================] - 991s 11s/step - loss: 0.0224 - accuracy: 0.9929 - val_loss: 0.0647 - val_accuracy: 0.9822\n",
      "Epoch 20/50\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9929 \n",
      "Epoch 00020: val_loss did not improve from 0.06108\n",
      "94/94 [==============================] - 989s 11s/step - loss: 0.0217 - accuracy: 0.9929 - val_loss: 0.0654 - val_accuracy: 0.9842\n",
      "Epoch 21/50\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9938 \n",
      "Epoch 00021: val_loss did not improve from 0.06108\n",
      "94/94 [==============================] - 990s 11s/step - loss: 0.0193 - accuracy: 0.9938 - val_loss: 0.0757 - val_accuracy: 0.9783\n",
      "Epoch 22/50\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9938 \n",
      "Epoch 00022: val_loss did not improve from 0.06108\n",
      "94/94 [==============================] - 989s 11s/step - loss: 0.0189 - accuracy: 0.9938 - val_loss: 0.0924 - val_accuracy: 0.9800\n",
      "Epoch 23/50\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9899 \n",
      "Epoch 00023: val_loss did not improve from 0.06108\n",
      "94/94 [==============================] - 994s 11s/step - loss: 0.0311 - accuracy: 0.9899 - val_loss: 0.0698 - val_accuracy: 0.9845\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(X_train, y_train, batch_size=256, epochs=50, validation_split=0.2, callbacks=[checkpoint, early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 70s 225ms/step - loss: 0.0776 - accuracy: 0.9828\n",
      "정확도 : 0.9828\n"
     ]
    }
   ],
   "source": [
    "print(\"정확도 : %.4f\" % (model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minhoAI",
   "language": "python",
   "name": "minhoai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
